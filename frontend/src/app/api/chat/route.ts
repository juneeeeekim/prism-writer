// =============================================================================
// PRISM Writer - Chat API (Refactored)
// =============================================================================
// íŒŒì¼: frontend/src/app/api/chat/route.ts
// ì—­í• : RAG ê¸°ë°˜ AI ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸
// ë¦¬íŒ©í† ë§: 2026-01-20 - 601ì¤„ â†’ ~200ì¤„
// =============================================================================

import { NextRequest, NextResponse } from 'next/server'
import { generateTextStream } from '@/lib/llm/gateway'
import { getModelForUsage } from '@/config/llm-usage-map'
import { createClient } from '@/lib/supabase/server'
import { verifyCitation, hasCitationMarkers } from '@/lib/rag/citationGate'
import { verifyGroundedness } from '@/lib/rag/selfRAG'
import { FEATURE_FLAGS } from '@/config/featureFlags'
import { type RubricTier } from '@/lib/rag/rubrics'
import {
  saveMessageWithRetry,
  searchUserPreferences,
  formatUserPreferences,
  searchTemplateContext,
  performRAGSearch,
  buildSystemPrompt,
  buildFullPrompt,
  touchSession,
  shouldRunLazySelfRAG,
} from '@/lib/services/chat'

export const runtime = 'nodejs'

// =============================================================================
// [P1-02] ìƒíƒœ ë©”ì‹œì§€ ìƒìˆ˜ ì •ì˜ (Progressive Streaming)
// =============================================================================
const STATUS_MESSAGES = {
  SEARCHING: '[STATUS]ğŸ” ìë£Œ ê²€ìƒ‰ ì¤‘...\n',
  GENERATING: '[STATUS]ğŸ“š ë‹µë³€ ìƒì„± ì¤‘...\n',
} as const

export async function POST(req: NextRequest) {
  const startTime = performance.now()

  try {
    // =========================================================================
    // 1. Request Parsing & Auth
    // =========================================================================
    const { messages, model: requestedModel, sessionId, projectId } = await req.json()
    const lastMessage = messages[messages.length - 1]
    const query = lastMessage.content

    const supabase = await createClient()
    const { data: { user } } = await supabase.auth.getUser()
    const userId = user?.id

    if (!userId) {
      return NextResponse.json(
        { error: 'Unauthorized', message: 'ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.' },
        { status: 401 }
      )
    }

    // Save user message
    if (sessionId && lastMessage.role === 'user') {
      await saveMessageWithRetry(supabase, {
        session_id: sessionId,
        role: 'user',
        content: lastMessage.content,
        model_id: requestedModel,
      })
    }

    // =========================================================================
    // 2. [FIX] Progressive Streaming - ì¦‰ì‹œ ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
    // 2026-01-21: Promise.allì„ ìŠ¤íŠ¸ë¦¼ ë‚´ë¶€ë¡œ ì´ë™
    // =========================================================================
    const modelId = requestedModel || getModelForUsage('rag.answer')

    const stream = new ReadableStream({
      async start(controller) {
        const encode = (text: string) => new TextEncoder().encode(text)
        let fullResponse = ''
        let firstTokenLogged = false

        try {
          // ---------------------------------------------------------------------
          // Step 1: ì¦‰ì‹œ ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡ (í´ë¼ì´ì–¸íŠ¸ íƒ€ì„ì•„ì›ƒ ë°©ì§€)
          // ---------------------------------------------------------------------
          controller.enqueue(encode(STATUS_MESSAGES.SEARCHING))

          // ---------------------------------------------------------------------
          // Step 2: Parallel Fetch - RAG ê²€ìƒ‰ (ìŠ¤íŠ¸ë¦¼ ë‚´ì—ì„œ ì‹¤í–‰)
          // ---------------------------------------------------------------------
          const [userPreferences, templateContext, ragResult] = await Promise.all([
            searchUserPreferences(userId, query),
            searchTemplateContext(supabase, userId, query),
            performRAGSearch(query, { userId, projectId }),
          ])

          console.log(`[Chat API] Parallel fetch: ${(performance.now() - startTime).toFixed(0)}ms`)

          // ---------------------------------------------------------------------
          // Step 3: Build Prompt
          // ---------------------------------------------------------------------
          const userPreferencesContext = formatUserPreferences(userPreferences)
          const { context, hasRetrievedDocs, uniqueResults } = ragResult

          const systemPrompt = buildSystemPrompt({
            userPreferences: userPreferencesContext,
            templateContext,
            ragContext: context,
          })

          const fullPrompt = buildFullPrompt(systemPrompt, messages)

          // ---------------------------------------------------------------------
          // Step 4: LLM ì‘ë‹µ ìƒì„± ìƒíƒœ ì „ì†¡
          // ---------------------------------------------------------------------
          controller.enqueue(encode(STATUS_MESSAGES.GENERATING))

          // ---------------------------------------------------------------------
          // Step 5: LLM Streaming (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
          // [2026-01-21] ë””ë²„ê·¸ ë¡œê¹… ì¶”ê°€ - LLM ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ì  ì¶”ì 
          // ---------------------------------------------------------------------
          console.log(`[Chat API] Starting LLM stream with model: ${modelId}`)
          
          let llmChunkCount = 0
          for await (const chunk of generateTextStream(fullPrompt, { model: modelId, context: 'rag.answer' })) {
            if (chunk.text) {
              llmChunkCount++
              if (!firstTokenLogged) {
                console.log(`[Chat API] TTFT: ${(performance.now() - startTime).toFixed(0)}ms`)
                firstTokenLogged = true
              }
              fullResponse += chunk.text
              controller.enqueue(encode(chunk.text))
            }
            if (chunk.done) break
          }
          
          console.log(`[Chat API] LLM stream completed. Chunks: ${llmChunkCount}, Total chars: ${fullResponse.length}`)

          // ---------------------------------------------------------------------
          // Step 6: Groundedness Check (Self-RAG) - [L2-02] Lazy Self-RAG ì ìš©
          // [2026-01-21] ì¡°ê±´ë¶€ ê²€ì¦: ê³ ìœ„í—˜ ì‘ë‹µì—ë§Œ Self-RAG ì‹¤í–‰
          // ---------------------------------------------------------------------
          const shouldVerify = shouldRunLazySelfRAG(query, fullResponse, hasRetrievedDocs)
          
          // [L4-01] ê²€ì¦ ìŠ¤í‚µ/ì‹¤í–‰ ë¡œê¹…
          console.log(`[Chat API] Lazy Self-RAG: ${shouldVerify ? 'VERIFY' : 'SKIP'}`, {
            queryLength: query.length,
            responseLength: fullResponse.length,
            hasRetrievedDocs,
          })
          
          if (shouldVerify && uniqueResults.length > 0 && fullResponse.length > 100) {
            const verification = await verifyGroundedness(fullResponse, uniqueResults, {
              supabase,
              userId,
              projectId,
            })

            if (!verification.isGrounded) {
              const warningMsg = '\n\nâš ï¸ ì£¼ì˜: ì¼ë¶€ ë‚´ìš©ì´ ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
              fullResponse += warningMsg
              controller.enqueue(encode(warningMsg))
            }
          }

          // ---------------------------------------------------------------------
          // Step 7: Save Assistant Message - ê¸°ì¡´ ë¡œì§ ìœ ì§€
          // ---------------------------------------------------------------------
          if (sessionId && fullResponse) {
            const citationMetadata = buildCitationMetadata(fullResponse, hasRetrievedDocs, uniqueResults)

            const saveSuccess = await saveMessageWithRetry(supabase, {
              session_id: sessionId,
              role: 'assistant',
              content: fullResponse,
              model_id: modelId,
              metadata: citationMetadata,
            })

            if (!saveSuccess) {
              const warningMsg = '\n\nâš ï¸ _ë©”ì‹œì§€ê°€ ì„œë²„ì— ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤._'
              controller.enqueue(encode(warningMsg))
            }

            await touchSession(supabase, sessionId)
          }

          controller.close()
        } catch (error: any) {
          // [2026-01-21] ì—ëŸ¬ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ë©”ì‹œì§€ ì „ì†¡ (ë””ë²„ê¹… ìš©ì´)
          console.error('Streaming error:', error)
          const errorMsg = error?.message || 'Unknown error'
          console.error('[Chat API] LLM Error Details:', JSON.stringify({
            message: errorMsg,
            name: error?.name,
            stack: error?.stack?.slice(0, 500),
          }))
          
          // ì‚¬ìš©ìì—ê²Œ ì—ëŸ¬ ë©”ì‹œì§€ ì „ì†¡ í›„ ì •ìƒ ì¢…ë£Œ
          try {
            controller.enqueue(encode(`\n\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${errorMsg}`))
            controller.close()
          } catch {
            controller.error(error)
          }
        }
      },
    })

    return new NextResponse(stream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    })
  } catch (error: any) {
    console.error('Chat API Error:', error)
    return NextResponse.json(
      { error: 'Internal Server Error', details: error.message },
      { status: 500 }
    )
  }
}

// =============================================================================
// Helper: Build Citation Metadata
// =============================================================================

function buildCitationMetadata(
  fullResponse: string,
  hasRetrievedDocs: boolean,
  uniqueResults: any[]
): Record<string, any> {
  if (!hasRetrievedDocs || !uniqueResults || uniqueResults.length === 0) {
    return {}
  }

  const sourceChunksForVerify = uniqueResults.map((r) => ({
    id: r.chunkId,
    content: r.content,
  }))

  const verificationResult = verifyCitation(fullResponse, sourceChunksForVerify)
  const hasMarkers = hasCitationMarkers(fullResponse)
  const adjustedScore = hasMarkers
    ? Math.min(verificationResult.matchScore + 0.15, 1.0)
    : verificationResult.matchScore

  const topResult = uniqueResults[0]
  const rubricTier = topResult?.metadata?.tier as RubricTier | undefined

  const sources = uniqueResults.slice(0, 5).map((r) => ({
    title: r.metadata?.title || 'Untitled',
    chunkId: r.chunkId,
    score: Math.round(r.score * 100) / 100,
  }))

  return {
    citation_verification: {
      ...verificationResult,
      matchScore: Math.round(adjustedScore * 100) / 100,
      valid: adjustedScore >= 0.7,
    },
    source_count: uniqueResults.length,
    rubric_tier: rubricTier,
    sources,
  }
}
